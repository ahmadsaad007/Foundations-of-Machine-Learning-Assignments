{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code below the TO-DO statements to finish the assignment. Keep the interfaces\n",
    "of the provided functions unchanged. Change the returned values of these functions\n",
    "so that they are consistent with the assignment instructions. Include additional import\n",
    "statements and functions if necessary.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "'''\n",
    "The loss functions shall return a scalar, which is the *average* loss of all the examples\n",
    "'''\n",
    "\n",
    "'''\n",
    "For instance, the square loss of all the training examples is computed as below:\n",
    "\n",
    "def squared_loss(train_y, pred_y):\n",
    "\n",
    "    loss = np.mean(np.square(train_y - pred_y))\n",
    "\n",
    "    return loss\n",
    "'''\n",
    "\n",
    "def logistic_loss(train_y, pred_y):\n",
    "    \"\"\"Calculates and returns the logistic loss\"\"\"\n",
    "    return np.mean(np.log(1+np.exp(-train_y*pred_y)))\n",
    "\n",
    "def hinge_loss(train_y, pred_y):\n",
    "    \"\"\"Calculates and returns the hinge loss\"\"\"\n",
    "    hinge = np.mean(np.maximum(0,(1-train_y*pred_y)))\n",
    "    return hinge\n",
    "\n",
    "'''\n",
    "The regularizers shall compute the loss without considering the bias term in the weights\n",
    "'''\n",
    "def l1_reg(w):\n",
    "    \n",
    "    return np.linalg.norm(w[:-1], ord=1)\n",
    "\n",
    "def l2_reg(w):\n",
    "\n",
    "    return np.linalg.norm(w[:-1], ord=2)\n",
    "\n",
    "def train_classifier(train_x, train_y, learn_rate, loss, lambda_val=None, regularizer=None):\n",
    "\n",
    "    \"\"\"create a weights vector of random normal values\"\"\"\n",
    "    weights =  np.random.normal(scale=0.01, size = len(train_x[0]))\n",
    "    numIters = 150\n",
    "    lossValues = []\n",
    "    \"\"\"Do Gradient descent numIters time\"\"\"\n",
    "    for i in range(numIters):\n",
    "        gradient = calculateGradient(weights, train_x, train_y, learn_rate, loss, lambda_val, regularizer)\n",
    "        #update the weights with each iteration using the gradient\n",
    "        weights = weights - learn_rate*gradient\n",
    "        \n",
    "        #lossValues.append(lossValue) \n",
    "        #I was returning lossValue as a return value from calculateGradient\n",
    "    #\"\"\"This is how I plotted the graphs\"\"\"\n",
    "    #plt.plot(range(numIters), lossValues, 'o', color='black');\n",
    "    #plt.xlabel('Number of Iterations')\n",
    "    #plt.ylabel('Loss Value')\n",
    "    #plt.show()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def calculateGradient(weights, train_x, train_y, learn_rate, loss, lambda_val, regularizer):\n",
    "    \"\"\"this function calculates the gradient and returns a gradient vector\"\"\"    \n",
    "    \n",
    "    \"\"\"create a score vector that stores the dot product of weights and examples\"\"\"\n",
    "    \"\"\"calculate the dot product for intitial prediction\"\"\"\n",
    "    score = np.zeros(len(train_x))\n",
    "    for i,x in enumerate(train_x,0):\n",
    "        score[i]=np.dot(weights, x)\n",
    "    firstTerm_base = 0\n",
    "    #Regularizer is None if it is Logistic Regression\n",
    "    if(regularizer!= None or lambda_val != None):\n",
    "        firstTerm_base = lambda_val*regularizer(weights)\n",
    "    secondTerm_base = loss(train_y, score)\n",
    "    \n",
    "    base_value = firstTerm_base +secondTerm_base\n",
    "    #loss value is used to plot a graph later\n",
    "    #lossValue = secondTerm_base\n",
    "    \n",
    "    \"\"\"Add h to each column of train_x and compute gradient again\"\"\"\n",
    "    \n",
    "    h = 0.0001\n",
    "    \n",
    "    gradient = np.zeros(len(weights))\n",
    "    #print(\"WEIGHTS\", weights)\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i]+h\n",
    "        #print(\"WEIGHTS: \",weights)\n",
    "        for j,x in enumerate(train_x,0):\n",
    "            score[j]=np.dot(weights, x)\n",
    "        firstTerm_new = 0\n",
    "        if(regularizer != None or lambda_val != None):\n",
    "            firstTerm_new = lambda_val*regularizer(weights)\n",
    "        secondTerm_new = loss(train_y, score)\n",
    "        new_value = firstTerm_new +secondTerm_new\n",
    "        \n",
    "        gradient[i] = (new_value - base_value)/h\n",
    "   \n",
    "    return gradient\n",
    "\n",
    "def test_classifier(w, test_x):\n",
    "    pred_y = np.dot(test_x, w)\n",
    "    return pred_y\n",
    "\n",
    "def compute_accuracy( pred_y, test_y):\n",
    "    correct = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if ((test_y[i] >0 and pred_y[i] > 0) or (test_y[i] < 0 and pred_y[i] < 0)):\n",
    "            correct += 1\n",
    "    return (correct/float(len(test_y))) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM: \n",
      "FOLD #: 1\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.55555555555556\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  70.37037037037037\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.92592592592592\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  80.18518518518518\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  58.14814814814815\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  77.4074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  77.4074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  66.11111111111111\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.55555555555556\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  64.81481481481481\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  64.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  65.37037037037037\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  73.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  75.0\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  42.77777777777778\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  63.888888888888886\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  73.14814814814815\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  22.037037037037038\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  50.0\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  48.888888888888886\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  47.03703703703704\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  34.25925925925926\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  57.592592592592595\n",
      "\n",
      "\n",
      "FOLD #: 2\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  68.33333333333333\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  76.66666666666667\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  76.11111111111111\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  78.51851851851852\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  38.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  74.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.22222222222223\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  76.66666666666667\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  75.0\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  75.37037037037037\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  66.66666666666666\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  76.85185185185185\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.03703703703704\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  61.85185185185185\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  58.51851851851851\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  58.7037037037037\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.5925925925926\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  40.18518518518518\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  56.481481481481474\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  42.77777777777778\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  54.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.29629629629629\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  29.444444444444446\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  40.74074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  43.888888888888886\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  39.81481481481482\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  42.77777777777778\n",
      "\n",
      "\n",
      "FOLD #: 3\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  73.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  74.07407407407408\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  78.51851851851852\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  80.0\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.81481481481481\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.5925925925926\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  72.96296296296296\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  75.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  75.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  74.81481481481481\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  74.62962962962963\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.55555555555556\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  77.4074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  75.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  57.22222222222222\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  57.03703703703704\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  57.592592592592595\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  69.81481481481482\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  26.481481481481485\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  45.74074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  53.51851851851852\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  62.22222222222222\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  54.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  74.62962962962963\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  37.407407407407405\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  53.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  58.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  47.77777777777778\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  52.03703703703704\n",
      "\n",
      "\n",
      "FOLD #: 4\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  34.074074074074076\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  55.37037037037037\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.92592592592592\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  83.14814814814815\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  83.14814814814815\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  77.77777777777779\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.33333333333333\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  78.51851851851852\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  76.85185185185185\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  75.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.70370370370371\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  61.48148148148148\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  59.62962962962963\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  60.74074074074074\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  44.25925925925926\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  22.407407407407405\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  76.11111111111111\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  53.51851851851852\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  55.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  48.148148148148145\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  22.22222222222222\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  33.14814814814815\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  52.96296296296297\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  45.55555555555556\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  50.92592592592593\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  48.51851851851852\n",
      "\n",
      "\n",
      "FOLD #: 5\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.11111111111111\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.14814814814814\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  81.66666666666667\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.1\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  81.85185185185185\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.01\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  40.18518518518518\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.14814814814814\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  58.88888888888889\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  57.96296296296296\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  57.77777777777777\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  79.81481481481481\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  54.81481481481482\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  54.81481481481482\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  69.81481481481482\n",
      "\n",
      "\n",
      "\tLearning Rate:  0.0001\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  66.85185185185185\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  31.11111111111111\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  57.77777777777777\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  45.370370370370374\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  34.44444444444444\n",
      "\n",
      "\n",
      "\tLearning Rate:  1e-05\n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  38.148148148148145\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Read the training data file\n",
    "    szDatasetPath = 'winequality-white.csv'\n",
    "    listClasses = []\n",
    "    listAttrs = []\n",
    "    bFirstRow = True\n",
    "    with open(szDatasetPath) as csvFile:\n",
    "        csvReader = csv.reader(csvFile, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if bFirstRow:\n",
    "                bFirstRow = False\n",
    "                continue\n",
    "            if int(row[-1]) < 6:\n",
    "                listClasses.append(-1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "            elif int(row[-1]) > 6:\n",
    "                listClasses.append(+1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "    \"\"\"Added a column of 1's to the data\"\"\"\n",
    "    for i in range(len(listAttrs)):\n",
    "        listAttrs[i].append(1.0)\n",
    "        \n",
    "    dataX = np.array(listAttrs)\n",
    "    dataY = np.array(listClasses)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "# Note: in this assignment, preprocessing the feature values will make\n",
    "# a big difference on the accuracy. Perform feature normalization after\n",
    "# spliting the data to training and validation set. The statistics for\n",
    "# normalization would be computed on the training set and applied on\n",
    "# training and validation set afterwards.\n",
    "# TO-DO: Add your code here\n",
    "\n",
    "        \n",
    "    #get a random permutation of elements in dataX\n",
    "    randX = []\n",
    "    randY = []\n",
    "    temp = np.arange(0, 2700, 1).tolist()\n",
    "    random.shuffle(temp)\n",
    "   \n",
    "    for i in temp:\n",
    "        randX.append(dataX[i])\n",
    "        randY.append(dataY[i])\n",
    "    randDataX = np.array(randX)\n",
    "    randDataY = np.array(randY)\n",
    "   \n",
    "    #perform k-folds on it\n",
    "    k = 5\n",
    "    foldsX = (np.array_split(randDataX, k))\n",
    "    foldsY = (np.array_split(randDataY, k))\n",
    "    \"\"\"Setting the hyper parameter for train_classifier\"\"\"\n",
    "    learningRate = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "    lambdaValue = [100,10,1,0.1,0.01,0.001]\n",
    "    print(\"\\nSVM: \")\n",
    "    \"\"\"Run a loop to get the folds. Fold[i] is the test data and remaining parts are the training data\"\"\"\n",
    "    for i in range(k):\n",
    "        \n",
    "        print(\"FOLD #:\", i+1)\n",
    "        for lr in learningRate:\n",
    "            \n",
    "            \n",
    "            for lmbda in lambdaValue:\n",
    "                \n",
    "                trainX = foldsX.copy() \n",
    "                trainY = foldsY.copy()\n",
    "                testX = foldsX[i]\n",
    "                testY = foldsY[i]\n",
    "                \"\"\"Deleting the test data from the aggregate to perform k-fold cross validation\"\"\"\n",
    "                del trainX[i]\n",
    "                del trainY[i]\n",
    "\n",
    "                trainX = np.concatenate(trainX,axis=0)\n",
    "                trainY = np.concatenate(trainY,axis=0)\n",
    "\n",
    "                \"\"\"Normalizing the training data by calculating the mean and std deviation of each feature value and \n",
    "                subtracting each value by the mean and dividing by std deviation\"\"\"\n",
    "                mean = []\n",
    "                std = []\n",
    "                nTrainX = []\n",
    "                for j in range(len(trainX[0])-1):\n",
    "                    columnList = trainX[:,j]\n",
    "                    mean.append(np.mean(columnList))\n",
    "                    std.append(np.std(columnList))\n",
    "                    columnList= columnList-mean[j]\n",
    "                    columnList = columnList/std[j]\n",
    "                    nTrainX.append(columnList)\n",
    "                nTrainX = np.array(nTrainX).T\n",
    "\n",
    "                \"\"\"Normalize the testing data\"\"\"\n",
    "                nTestX = []\n",
    "                for j in range(len(trainX[0])-1):\n",
    "                    columnList = testX[:,j]\n",
    "                    columnList= columnList-mean[j]\n",
    "                    columnList = columnList/std[j]\n",
    "                    nTestX.append(columnList)\n",
    "                nTestX = np.array(nTestX).T\n",
    "                #Logistic Regression\n",
    "                #weights = train_classifier(nTrainX, trainY, lr, logistic_loss)\n",
    "                #nPredY= test_classifier(weights,nTestX)\n",
    "                #print(\"\\tLearning Rate: \", lr)\n",
    "                #print(\"\\t\\tAccuracy: \", compute_accuracy(nPredY, testY))\n",
    "                \n",
    "                #SVM\n",
    "                weights = train_classifier(nTrainX, trainY, lr, hinge_loss, lmbda, l2_reg)\n",
    "                nPredY= test_classifier(weights,nTestX)\n",
    "                print(\"\\tLearning Rate: \", lr)\n",
    "                print(\"\\tLambda Value: \", lmbda)\n",
    "                print(\"\\tAccuracy: \", compute_accuracy(nPredY, testY))\n",
    "                print(\"\\n\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
