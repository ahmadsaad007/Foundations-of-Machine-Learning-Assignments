{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add the code below the TO-DO statements to finish the assignment. Keep the interfaces\n",
    "of the provided functions unchanged. Change the returned values of these functions\n",
    "so that they are consistent with the assignment instructions. Include additional import\n",
    "statements and functions if necessary.\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "'''\n",
    "The loss functions shall return a scalar, which is the *average* loss of all the examples\n",
    "'''\n",
    "\n",
    "'''\n",
    "For instance, the square loss of all the training examples is computed as below:\n",
    "\n",
    "def squared_loss(train_y, pred_y):\n",
    "\n",
    "    loss = np.mean(np.square(train_y - pred_y))\n",
    "\n",
    "    return loss\n",
    "'''\n",
    "\n",
    "def logistic_loss(train_y, pred_y):\n",
    "\n",
    "    # TO-DO: Add your code here\n",
    "\n",
    "    return np.mean(np.log(1+np.exp(-train_y*pred_y)))\n",
    "\n",
    "def hinge_loss(train_y, pred_y):\n",
    "   \n",
    "    # TO-DO: Add your code here\n",
    "    hinge = np.mean(np.maximum(0,(1-train_y*pred_y)))\n",
    "    return hinge\n",
    "\n",
    "'''\n",
    "The regularizers shall compute the loss without considering the bias term in the weights\n",
    "'''\n",
    "def l1_reg(w):\n",
    "\n",
    "    # TO-DO: Add your code here\n",
    "\n",
    "    return np.linalg.norm(w[:-1], ord=1)\n",
    "\n",
    "def l2_reg(w):\n",
    "\n",
    "    # TO-DO: Add your code here\n",
    "\n",
    "    return np.linalg.norm(w[:-1], ord=2)\n",
    "\n",
    "def train_classifier(train_x, train_y, learn_rate, loss, lambda_val=None, regularizer=None):\n",
    "\n",
    "    # TO-DO: Add your code here\n",
    "    \"\"\"create a weights vector of random normal values\"\"\"\n",
    "    weights =  np.random.normal(scale=0.01, size = len(train_x[0]))\n",
    "    numIters = 150\n",
    "    \"\"\"Do Gradient descent numIters time\"\"\"\n",
    "    for i in range(numIters):\n",
    "        gradient = calculateGradient(weights, train_x, train_y, learn_rate, loss, lambda_val, regularizer)\n",
    "        #update the weights with each iteration using the gradient\n",
    "        weights = weights - learn_rate*gradient\n",
    "    return weights\n",
    "\n",
    "def calculateGradient(weights, train_x, train_y, learn_rate, loss, lambda_val, regularizer):\n",
    "    \"\"\"this function calculates the gradient and returns a gradient vector\"\"\"    \n",
    "    \n",
    "    \"\"\"create a score vector that stores the dot product of weights and examples\"\"\"\n",
    "    \"\"\"calculate the dot product for intitial prediction\"\"\"\n",
    "    score = np.zeros(len(train_x))\n",
    "   \n",
    "    for i,x in enumerate(train_x,0):\n",
    "        score[i]=np.dot(weights, x)\n",
    "    firstTerm_base = 0\n",
    "    #Regularizer is None if it is Logistic Regression\n",
    "    if(regularizer!= None or lambda_val != None):\n",
    "        firstTerm_base = lambda_val*regularizer(weights)\n",
    "    secondTerm_base = loss(train_y, score)#/len(train_x)\n",
    "    base_value = firstTerm_base +secondTerm_base\n",
    "   \n",
    "    \"\"\"Add h to each column of train_x and compute gradient again\"\"\"\n",
    "    h = 0.0001\n",
    "    gradient = np.zeros(len(weights))\n",
    "    #print(\"WEIGHTS\", weights)\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i]+h\n",
    "        #print(\"WEIGHTS: \",weights)\n",
    "        for j,x in enumerate(train_x,0):\n",
    "            score[j]=np.dot(weights, x)\n",
    "        firstTerm_new = 0\n",
    "        if(regularizer != None or lambda_val != None):\n",
    "            firstTerm_new = lambda_val*regularizer(weights)\n",
    "        secondTerm_new = loss(train_y, score)\n",
    "        new_value = firstTerm_new +secondTerm_new\n",
    "        \n",
    "        gradient[i] = (new_value - base_value)/h\n",
    "   \n",
    "    return gradient\n",
    "   \n",
    "def test_classifier(w, test_x):\n",
    "\n",
    "    # TO-DO: Add your code here\n",
    "    pred_y = np.dot(test_x, w)\n",
    "    return pred_y\n",
    "\n",
    "def compute_accuracy( pred_y, test_y):\n",
    "    correct = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if ((test_y[i] >0 and pred_y[i] > 0) or (test_y[i] < 0 and pred_y[i] < 0)):\n",
    "            correct += 1\n",
    "    return (correct/float(len(test_y))) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.4074074074074\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  80.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  38.148148148148145\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.0\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.0\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  73.14814814814815\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  80.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  77.77777777777779\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.0\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.22222222222223\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  80.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.51851851851852\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.03703703703704\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.81481481481481\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.92592592592592\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  80.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  81.11111111111111\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  81.66666666666667\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  81.11111111111111\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  79.25925925925927\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  80.37037037037037\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  80.0\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  80.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  81.48148148148148\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  81.66666666666667\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  81.2962962962963\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.1 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  80.37037037037037\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  67.77777777777779\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.03703703703704\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.74074074074073\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  41.2962962962963\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  69.81481481481482\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.48148148148148\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  69.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  55.37037037037037\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  71.85185185185186\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  70.92592592592592\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  67.4074074074074\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.74074074074075\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  70.18518518518519\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  69.81481481481482\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.44444444444444\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  69.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  74.62962962962963\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  71.66666666666667\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  79.81481481481481\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  67.96296296296296\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  75.37037037037037\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  70.18518518518519\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.14814814814814\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  70.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.5925925925926\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  69.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.77777777777779\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  71.48148148148148\n",
      "\n",
      "SVM: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.88888888888889\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  67.77777777777779\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  77.22222222222223\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  70.74074074074073\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  69.25925925925925\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  79.07407407407408\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  69.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  75.92592592592592\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  71.29629629629629\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  78.33333333333333\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  68.51851851851852\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  75.37037037037037\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  70.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  77.03703703703704\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  70.18518518518519\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  77.77777777777779\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  69.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  75.55555555555556\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  71.29629629629629\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  67.96296296296296\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  75.37037037037037\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  70.18518518518519\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  76.85185185185185\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  69.81481481481482\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  69.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  75.37037037037037\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  70.92592592592592\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.01 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  78.33333333333333\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  56.481481481481474\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  73.14814814814815\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  77.22222222222223\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  58.88888888888889\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  75.55555555555556\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  62.59259259259259\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.5925925925926\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  56.2962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  40.74074074074074\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  57.96296296296296\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.0\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.51851851851852\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  62.03703703703704\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.55555555555556\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  60.92592592592593\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  75.92592592592592\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  61.48148148148148\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  80.74074074074075\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  60.55555555555555\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  76.66666666666667\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  58.333333333333336\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  58.88888888888889\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.4074074074074\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  60.74074074074074\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  77.03703703703704\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  57.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  78.88888888888889\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  55.00000000000001\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  58.51851851851851\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  60.18518518518518\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  58.333333333333336\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  63.14814814814815\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  61.85185185185185\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  62.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  62.40740740740741\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  64.07407407407408\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  62.77777777777778\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  58.14814814814815\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  57.77777777777777\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  60.55555555555555\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  55.00000000000001\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  59.25925925925925\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  60.55555555555555\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  62.59259259259259\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  61.48148148148148\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  58.333333333333336\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  56.111111111111114\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  63.33333333333333\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  59.25925925925925\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  61.66666666666667\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  57.77777777777777\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  64.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  59.81481481481481\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  61.48148148148148\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  59.62962962962963\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  41.66666666666667\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  61.29629629629629\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  37.592592592592595\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  21.296296296296298\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  38.88888888888889\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  58.7037037037037\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  41.2962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  37.03703703703704\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  52.77777777777778\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  59.074074074074076\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  39.81481481481482\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  67.4074074074074\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  34.81481481481482\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  77.96296296296296\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  40.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  70.37037037037037\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  52.77777777777778\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  70.92592592592592\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  58.14814814814815\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  78.88888888888889\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  43.333333333333336\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  71.11111111111111\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  36.2962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  52.77777777777778\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  38.51851851851852\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  38.333333333333336\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  42.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  71.48148148148148\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  44.25925925925926\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  50.18518518518519\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  37.03703703703704\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  48.333333333333336\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  35.74074074074074\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  69.44444444444444\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  52.22222222222223\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  57.592592592592595\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  47.96296296296296\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  53.333333333333336\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  42.592592592592595\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  48.888888888888886\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  41.48148148148148\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  53.333333333333336\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  56.111111111111114\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  66.85185185185185\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  66.48148148148148\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  61.48148148148148\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  52.407407407407405\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  56.111111111111114\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  43.148148148148145\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  61.66666666666667\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  40.370370370370374\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  56.851851851851855\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  42.77777777777778\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  46.666666666666664\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  56.481481481481474\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  62.77777777777778\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  61.85185185185185\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  56.666666666666664\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  40.925925925925924\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  0.0001 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  32.592592592592595\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  41.11111111111111\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  22.77777777777778\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  46.2962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  76.11111111111111\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  48.148148148148145\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  72.77777777777777\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  39.62962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  77.5925925925926\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  42.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  100\n",
      "\tAccuracy:  78.14814814814814\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  42.592592592592595\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  65.0\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  40.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  26.111111111111114\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  48.333333333333336\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  50.92592592592593\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  46.666666666666664\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  31.11111111111111\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  37.407407407407405\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  10\n",
      "\tAccuracy:  24.62962962962963\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  58.14814814814815\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  45.18518518518518\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  55.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  53.70370370370371\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  50.37037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  37.03703703703704\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  52.96296296296297\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  38.88888888888889\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  47.77777777777778\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  1\n",
      "\tAccuracy:  42.592592592592595\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  46.2962962962963\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  39.25925925925926\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  43.7037037037037\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  44.44444444444444\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  51.11111111111111\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  49.81481481481482\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  48.888888888888886\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  38.7037037037037\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  45.18518518518518\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.1\n",
      "\tAccuracy:  60.18518518518518\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  45.925925925925924\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  40.0\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  47.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  41.48148148148148\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  42.407407407407405\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  47.03703703703704\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  40.55555555555556\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  33.51851851851852\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  40.370370370370374\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.01\n",
      "\tAccuracy:  39.25925925925926\n",
      "\n",
      "\bFOLD #: 0\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  54.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  55.74074074074075\n",
      "\n",
      "\bFOLD #: 1\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  41.48148148148148\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  44.62962962962963\n",
      "\n",
      "\bFOLD #: 2\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  37.22222222222222\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  60.74074074074074\n",
      "\n",
      "\bFOLD #: 3\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  39.44444444444444\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  39.81481481481482\n",
      "\n",
      "\bFOLD #: 4\n",
      "\n",
      "Logistic Regression: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  45.925925925925924\n",
      "\n",
      "SVM: \n",
      "\tLearning Rate:  1e-05 \n",
      "\tLambda Value:  0.001\n",
      "\tAccuracy:  42.592592592592595\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Read the training data file\n",
    "    szDatasetPath = 'winequality-white.csv'\n",
    "    listClasses = []\n",
    "    listAttrs = []\n",
    "    bFirstRow = True\n",
    "    with open(szDatasetPath) as csvFile:\n",
    "        csvReader = csv.reader(csvFile, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if bFirstRow:\n",
    "                bFirstRow = False\n",
    "                continue\n",
    "            if int(row[-1]) < 6:\n",
    "                listClasses.append(-1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "            elif int(row[-1]) > 6:\n",
    "                listClasses.append(+1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "\n",
    "    dataX = np.array(listAttrs)\n",
    "    dataY = np.array(listClasses)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "# Note: in this assignment, preprocessing the feature values will make\n",
    "# a big difference on the accuracy. Perform feature normalization after\n",
    "# spliting the data to training and validation set. The statistics for\n",
    "# normalization would be computed on the training set and applied on\n",
    "# training and validation set afterwards.\n",
    "# TO-DO: Add your code here\n",
    "\n",
    "   \n",
    "    #get a random permutation of elements in dataX\n",
    "    randX = []\n",
    "    randY = []\n",
    "    temp = np.arange(0, 2700, 1).tolist()\n",
    "    random.shuffle(temp)\n",
    "   \n",
    "    for i in temp:\n",
    "        randX.append(dataX[i])\n",
    "        randY.append(dataY[i])\n",
    "    randDataX = np.array(randX)\n",
    "    randDataY = np.array(randY)\n",
    "   \n",
    "    #perform k-folds on it\n",
    "    k = 5\n",
    "    foldsX = (np.array_split(randDataX, k))\n",
    "    foldsY = (np.array_split(randDataY, k))\n",
    "    \"\"\"Setting the hyper parameter for train_classifier\"\"\"\n",
    "    learningRate = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "    lambdaValue = [100,10,1,0.1,0.01,0.001]\n",
    "    \n",
    "    for lr in learningRate:\n",
    "        \n",
    "        for lmbda in lambdaValue:\n",
    "            \n",
    "            \"\"\"Run a loop to get the folds. Fold[i] is the test data and remaining parts are the training data\"\"\"\n",
    "            for i in range(k):\n",
    "                print(\"\\n\\bFOLD #:\", i)\n",
    "                trainX = foldsX.copy() \n",
    "                trainY = foldsY.copy()\n",
    "                testX = foldsX[i]\n",
    "                testY = foldsY[i]\n",
    "                \"\"\"Deleting the test data from the aggregate to perform k-fold cross validation\"\"\"\n",
    "                del trainX[i]\n",
    "                del trainY[i]\n",
    "\n",
    "                trainX = np.concatenate(trainX,axis=0)\n",
    "                trainY = np.concatenate(trainY,axis=0)\n",
    "\n",
    "                \"\"\"Normalizing the training data by calculating the mean and std deviation of each feature value and \n",
    "                subtracting each value by the mean and dividing by std deviation\"\"\"\n",
    "                mean = []\n",
    "                std = []\n",
    "                nTrainX = []\n",
    "                #print(\"Orig Dat: \", trainX)\n",
    "                for j in range(len(trainX[0])):\n",
    "                    columnList = trainX[:,j]\n",
    "                    mean.append(np.mean(columnList))\n",
    "                    std.append(np.std(columnList))\n",
    "                    columnList= columnList-mean[j]\n",
    "                    columnList = columnList/std[j]\n",
    "                    nTrainX.append(columnList)\n",
    "                nTrainX = np.array(nTrainX).T\n",
    "\n",
    "                \"\"\"Normalize the testing data\"\"\"\n",
    "                nTestX = []\n",
    "                for j in range(len(trainX[0])):\n",
    "                    columnList = testX[:,j]\n",
    "                    columnList= columnList-mean[j]\n",
    "                    columnList = columnList/std[j]\n",
    "                    nTestX.append(columnList)\n",
    "                nTestX = np.array(nTestX).T\n",
    "                print(\"\\nLogistic Regression: \")\n",
    "                weights = train_classifier(nTrainX, trainY, lr, logistic_loss)\n",
    "                nPredY= test_classifier(weights,nTestX)\n",
    "                print(\"\\tLearning Rate: \", lr, \"\\n\\tLambda Value: \", lmbda)\n",
    "                print(\"\\tAccuracy: \", compute_accuracy(nPredY, testY))\n",
    "                \n",
    "                print(\"\\nSVM: \")\n",
    "                weights = train_classifier(nTrainX, trainY, lr, hinge_loss, lmbda, l2_reg)\n",
    "                nPredY= test_classifier(weights,nTestX)\n",
    "                print(\"\\tLearning Rate: \", lr, \"\\n\\tLambda Value: \", lmbda)\n",
    "                print(\"\\tAccuracy: \", compute_accuracy(nPredY, testY))\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is testing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "    k = 2\n",
    "    randDataX = np.array([[1,2,3,4,5,6,7,8,9,10],\n",
    "                         [11,12,13, 14,15,16,17,18,19,20],[1,2,3,4,5,6,7,8,9,10],\n",
    "                         [11,12,13, 14,15,16,17,18,19,20]])\n",
    "    randDataY = np.array([1,0,1,1,1,0,1,0,0,0,1,0,1,1,1,0,1,0,0,0])\n",
    "    foldsX = (np.array_split(randDataX, k))\n",
    "    foldsY = (np.array_split(randDataY, k))\n",
    "    \n",
    "    for i in range(k):\n",
    "        trainX = foldsX.copy() # you wanna work on a copy of your array\n",
    "        trainY = foldsY.copy()\n",
    "        testX = foldsX[i]\n",
    "        testY = foldsY[i]\n",
    "        del trainX[i]\n",
    "        del trainY[i]\n",
    "        \n",
    "        trainX = np.concatenate(trainX,axis=0)\n",
    "        trainY = np.concatenate(trainY,axis=0)\n",
    "        \n",
    "        print(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
